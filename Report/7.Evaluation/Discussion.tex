\textit{In this chapter, we will discuss different aspects of the project. We will evaluate and give an overall status on the project requirements discussed in the problem statement. Also, the experiments will be discussed and reflected upon. Furthermore, we will reflect on how the dataset was pre-processed and what consequences it might have had.}

\section{Requirements Evaluation}

In \textbf{\autoref{sec:requirements}}, we presented the requirements for this project. To summarize our process, we have presented the same requirements in \textbf{\autoref{table:requirements2}} in addition with a completion status. \textit{System} refers to the hybrid approach, where \gls{pdr} is used as the primary algorithm, and \gls{gbdt} as the secondary.

\vspace{12pt}
\begin{table}[H]
\caption{The requirements with index number, MoSCoW classification and a completion status. \textbf{F}, \textbf{PF}, and \textbf{NF} stand for Fulfilled, Partially Fulfilled, and Not Fulfilled, respectively.}
\begin{tabularx}{\textwidth}{| c | c | X | c | c | c |}
\hline
\textbf{Index} & \textbf{MoSCoW} & \textbf{Requirements} & \textbf{F} & \textbf{PF} & \textbf{NF}\\\hline

R.1 & M & The system must estimate the indoor location (floor level, x- and y-coordinates). & \checkmark  & & \\\hline
R.2 & M & The system must make use of data collected on a smartphone. & \checkmark & & \\\hline
R.3 & M & The data must be processed and cleaned to fit the positioning methods. & \checkmark & &\\\hline
R.4 & C & The system could be implemented as an \gls{ips} for smartphones. & & \checkmark & \\\hline
R.5 & C & The system could make use of location fingerprinting. & \checkmark & &\\\hline
R.6 & C & The method could incorporate \gls{imu}-based method(s). & \checkmark & &\\\hline
%R.7 & C & The system could be implemented as an Android service.\\\hline
R.7 & C & The system could have a graphical user interface. & & & \checkmark \\\hline
R.8 & W & The system is wanted to be deployable on smartphones. & & & \checkmark \\\hline
\end{tabularx}
\label{table:requirements2}
\end{table}

\begin{longtable}{p{.05\textwidth} p{.91\textwidth}}

\textbf{R.1} & This requirement is fulfilled, and the results are shown in \textbf{\autoref{tab:alg_performances}}. Here, it is shown how \gls{pdr} as primary can estimate the X-coordinate with an \gls{rmse} of 10.85 meter, the Y-coordinate with an \gls{rmse} of 10.9 meter, and the floor level with an \gls{rmse} of 0.52 meter.
\\\\

\textbf{R.2} & This requirement is fulfilled by the use of the dataset given by Kaggle. As described in \textbf{\autoref{sec:dataset}}, the data includes different types of data collected on a smartphone. This data is also used in our system in the form of \gls{imu} data and Wi-Fi data.
\\\\

\textbf{R.3} & As mentioned in \textbf{\autoref{sec:dataPreprocessing}}, prepossessing and cleaning of the data has been done. The relevant data is saved in \textit{.txt} files with standardised floor levels and the relevant information about the sites. Furthermore, in the feature engineering phase, we format the data to the required format for the positioning methods. The data is formatted for \gls{imu}-based methods, \gls{lfp} methods, time series-based methods, and for the our hybrid implementations.
\\\\

\textbf{R.4} & This requirement is set as partially fulfilled based on the fact that we have implemented a server, as mentioned in \textbf{\autoref{sec:final_architecture}}, but we have not implemented the client-side. We deem our solution as an indoor positioning system, since it is possible to estimate a position when given a specific type of data. 
\\\\

\textbf{R.5} & In our final solution, we use location fingerprinting as our secondary algorithm in our hybrid approach, and therefore, this requirement is deemed as fulfilled.
\\\\

\textbf{R.6} & As we use \gls{pdr} in our hybrid solution, which is an \gls{imu}-based method, this requirement is also deemed as fulfilled.
\\\\

\textbf{R.7} & We did not manage to fulfill this requirement, but we consider this reasonable as this requirement is classified as could have. 
\\\\

\textbf{R.8} & As mentioned above in \textbf{R.4}, we have implemented the server, but we did not manage to finish the client-side. We have implemented the possibility to create Docker containers for deployment, but we have not finished the deployment for smartphones. 
\\\\

\end{longtable}
\vspace{-24pt}

\section{Data}
After conducting the different experiments, certain consideration regarding the data can be made. Utilising different data or experimenting more with the feature engineering might lead to a dataset, on which it could be easier for the different models to classify more accurate locations.

\subsection{Threshold for \gls{bssid} values}
The first consideration is regarding the threshold on the \gls{bssid} values of 1000. This means that a \gls{bssid} value that is not present at least 1000 times is disregarded from the training data. The reason for choosing the occurrence threshold of 1000 was due to the other participants at the Kaggle Indoor Location \& Navigation competition. One of these participants can be seen at \cite{BSSID1000}. Though, others have utilised this threshold, we could still have experimented with increasing and decreasing this threshold to see if it would affect the performance of the model.

Applying this threshold for occurrence is only applied to the train data, which means all \gls{bssid}s are present in the test data. Not applying the threshold to the test data was to ensure the presence of \gls{bssid}s. Another experiment could be to apply the threshold to the test data as well to possibly increase the performance of the models.

\subsection{Alternative Data Sources}
Another possibility for changing the data was to use other types of data from the provided data by Kaggle. We chose to focus on working with Wi-Fi features, but utilising for example Bluetooth data could also be a possibility. The algorithm used to compute the Wi-Fi features could also work for converting the Bluetooth data by extracting the data from Kaggle with \textit{TYPE\_BLUETOOTH} instead of \textit{TYPE\_WIFI}. This could also be simple to test with the models.

It could also be interesting to use the \gls{imu} data for the different machine learning experiments. Currently it is only used for the \gls{pdr} experiment. Utilising this data for the machine learning models could increase performance and is therefore interesting to experiment with as an alternative data source for the machine learning models.

\subsection{One-hot Encoding}
An alternative approach to attempt to increase the performance of the model could be to apply One-hot encoding, which is a standard approach for categorical data. The approach works by converting labels of data into binary features. A reason for applying this is due to some machine learning algorithms not being able to work optimal directly on categorical data.

Decision trees do not have the difficulties of working with label data, therefore, this might only be interesting to experiment with for the \gls{ann} and \gls{rnn} models. Though, since we work with labels for floor that are of natural ordering, this might not have a huge impact on the results.

\subsection{Post Processing}
Alternatively, it could also have been interesting to experiment with utilising the technique \textit{Snap to Grid}. This technique was also utilised by the winner of the Kaggle's Indoor Navigation \& Location competition. The idea behind the technique is to use the waypoints from the training data to create a grid, and then place our predictions in them. We do this by calculating the closest grid point, which should be closer than a certain threshold and then snap it to the grid. This threshold is deemed to be most optimal between 3-8 by the competitors.

%\begin{itemize}
    %\item categorical vs numerical output (snap to grid)
    %\item one hot encoding
%\end{itemize}

\section{Gradient Boosting Decision Trees}

For the \gls{gbdt} and \gls{dart} implementations, we decided to use \gls{gbdt} rather then \gls{dart} due to the high resources required by \gls{dart}. However, \gls{dart} had more promising results in terms of less overfitting. If we had allocated more resources to this algorithm, \gls{dart} could potentially outperform \gls{gbdt}, but the resources was required elsewhere. This leads to the conclusion that a way to get a better solution could be to fit the \gls{dart} implementation better to the project. As we are only evaluating our final models on a subset of the total amount of sites, we could have chosen a subset of sites, for which the \gls{dart} algorithm behaves reasonably, which would make it possible to use \gls{dart} in our hybrid approach. %If we could eliminate sites with a bad performance and get an even better result, the \gls{dart} implementation seems to be a better alternative because it is handling overfitting more effectively. 

To test the \gls{dart} implementation, we have used Kaggle notebooks in which we could execute programs. The notebook has a time limit of 9 hours before automatically terminating the program. The \gls{dart} implementation, however, have a computation time exceeding this threshold. To circumvent this threshold, we have created 4 different notebooks where each notebook handles a part of the computation each. When each notebook has handled their models and predictions, a fifth notebook manage the results and models. This is a tedious process and makes it difficult to handle more than one test simultaneously. Testing \gls{dart} had been more manageable if we had allocated time for this process to run on another service, where the implementation had time to train the model with more resources and/or no time limit.

\section{Hybrids}
The position estimation from the average hybrid described in \textbf{\autoref{sec:hybrid_theory}} uses an average calculation between the position estimation from the \gls{pdr} algorithm and \gls{gbdt} model. Instead of simply computing the average, we could have experimented with different schemes, such as weighted average or different machine learning classifiers for the combination phase, which might better capture the interactions between the algorithms.

For the two hybrid approaches having \gls{pdr} as primary and the average hybrid, the number of measurements gathered before re-calibrating the \gls{pdr} algorithm could have been adjusted by experimentation, instead of choosing the value from the experiment results of \gls{pdr} and \gls{gbdt}.

As explained in \textbf{\autoref{sec:hybrid_theory}}, the performance of the \gls{pdr} algorithm is dependent on the \gls{lfp} algorithm at re-calibration time. Instead of assuming the position estimation of the \gls{lfp} algorithm is accurate at re-calibration time, we could have gathered multiple position estimations using the \gls{lfp} algorithm(s) before re-calibrating \gls{pdr} and use the latest position estimation that is closest to the majority of those gathered position estimations.

% Different schemes for combining pdr and lfp instead of just taking an average (weighted average or different machine learning classifier for the combination phase)
\section{Use Cases of the Solution}
In this project, we have mainly focused on the indoor positioning technology given that we have the necessary data from an indoor environment. However, to make our solution work as a whole service or system, some additional components are necessary.

%Our solution can be used in different use cases.
To properly utilise our solution as a general positioning system, the site, which needs location estimations, needs to be known. This is due to our solution providing indoor positioning specific to each site. This can be accomplished by using \gls{gps}. A bounding box can be defined for each site, which would require two \gls{gps} points. The bounding box is a rectangular area encapsulating \gls{gps} points. The \gls{gps} points for each site is also available in the dataset. Giving this bounding box, it would then be possible to identify the closest site given the last known \gls{gps} point, and then we can provide the estimated location from our solution.

Another use case would be from the perspective of separate corporations who need an indoor positioning system for their own location-based services or applications for their customers. The ones hosting the dataset on Kaggle are this type of stakeholder. In this use case, it might not be necessary to distinguish between different sites as they could only have one. In this case, they can more directly make use of the system without the need for an additional component to determine the site.